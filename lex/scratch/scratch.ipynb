{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Speed up steps\n",
    "\n",
    "- bf16  # with torch.autocast(device_type=device, dtype=torch.bfloat16): y = model(x)\n",
    "- torch.compile  # model = torch.compile(model)\n",
    "- vocab size in powers of 2\n",
    "- Use fused=True in Adamw\n",
    "\n",
    "Gotchas:\n",
    "- Weight decay only embeddings and matmul weights (not a speed-up, but something new)\n",
    "- When using DDP, dataloader should not load the same data for all GPUs\n",
    "- When using DDP, model should be created with same parameters. Either by setting a seed or loading from checkpoint.\n",
    "- \n",
    "\n",
    "## Train on one batch\n",
    "\n"
   ],
   "id": "65e2815d2b9222e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T04:56:13.301938Z",
     "start_time": "2024-08-17T04:56:13.256555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%reload_ext autoreload\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary"
   ],
   "id": "dde2ef6efb73adc8",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T04:56:13.662629Z",
     "start_time": "2024-08-17T04:56:13.653672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set device  [cuda:1, mps, cpu]\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:1\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "device"
   ],
   "id": "5199ad7d70bf2f3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T04:56:17.851039Z",
     "start_time": "2024-08-17T04:56:17.844864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ],
   "id": "28d4387a9e82d1c6",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T04:57:00.210130Z",
     "start_time": "2024-08-17T04:56:59.968477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import LlamaTokenizerFast\n",
    "enc = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\") # 32000\n",
    "enc"
   ],
   "id": "175d66dc347db147",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = "
   ],
   "id": "2999fdfee28ef18c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T19:24:54.878877Z",
     "start_time": "2024-08-08T19:24:54.871712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate(model, tokens, n=10):\n",
    "    tokens = deepcopy(tokens)\n",
    "    for i in range(n):\n",
    "        logits, _ = model(tokens.to(\"cuda:1\"))  # (B, T, vocab_size)\n",
    "        # We only care out last token\n",
    "        next_token_logits = logits[:, -1, :].to(\"cpu\")  # (B, vocab_size)\n",
    "        probs = F.softmax(next_token_logits, dim=-1)  # (B, vocab_size)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)  # (B, k)\n",
    "        # Sample\n",
    "        next_token_indices = torch.multinomial(topk_probs, num_samples=1)  # (B, 1)\n",
    "        next_token = torch.gather(topk_indices, -1, next_token_indices)  # (B, 1)\n",
    "        tokens = torch.cat([tokens, next_token], dim=-1)\n",
    "    return [enc.decode(ts.tolist()) for ts in tokens]"
   ],
   "id": "464eafa02119b9e2",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T19:24:55.378579Z",
     "start_time": "2024-08-08T19:24:54.880106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generate(model, tokens)"
   ],
   "id": "5f28c31f7e1fb3d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, I\\'m Indian, my name is Gopal,\" I said, saying as I looked',\n",
       " \"Hello, I'm Indian, my name is Chandi Gupta. I am a resident of the\",\n",
       " \"Hello, I'm Indian, my name is Rajan Gandhi and my mother's name is Ch\",\n",
       " \"Hello, I'm Indian, my name is Ajay. My daughter was born on November 28\",\n",
       " \"Hello, I'm Indian, my name is Raj Kumar Sharma and I work for Aichal\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-08T19:24:55.907373Z",
     "start_time": "2024-08-08T19:24:55.380501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mini_model = GPT(GPTConfig(block_size=512, vocab_size=tokenizer.n_vocab, n_layer=4, n_head=2, n_embd=128))\n",
    "generate(mini_model.to(\"cuda:1\"), tokens)"
   ],
   "id": "78de9cdea84c3a92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello, I'm Indian, my name isEconomic spells delegatessburgh Desktop681PE PSU Synd aggrav\",\n",
       " \"Hello, I'm Indian, my name is Celeb ranculous baconivably Spemare sidelineoxin elbows\",\n",
       " \"Hello, I'm Indian, my name is golden Int dorm Pai Participants Yi surprisinglyotton Candleatech\",\n",
       " \"Hello, I'm Indian, my name is Guarant Wy catchy Made 2015dfxmonitorAttribute boss presence\",\n",
       " \"Hello, I'm Indian, my name is correlation don optim Farmer Platoarro agree Jiusatotaur\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
