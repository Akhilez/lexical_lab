{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "use huggingface's AutoTokenizer.from_pretrained to find the smallest good tokenizer.\n",
    "\n",
    "\n",
    "- AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")  # 30522  Lossy\n",
    "- GPT2Tokenizer.from_pretrained(\"gpt2\")  # 50257\n",
    "- AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")  # 49408  # lossy\n",
    "- LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\") # 32000\n",
    "- \n",
    "\n"
   ],
   "id": "7a37fd2360947ff2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T22:47:30.596193Z",
     "start_time": "2024-08-13T22:47:30.591819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer"
   ],
   "id": "d3be9002e340089f",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T23:06:39.807793Z",
     "start_time": "2024-08-13T23:06:39.679427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = transformers.LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "tokenizer"
   ],
   "id": "8425e2d420c90231",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T23:06:40.013508Z",
     "start_time": "2024-08-13T23:06:40.003463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "string = \"Hello!!! This is a test to see how well the tokenizer works. :)\"\n",
    "tokens = tokenizer.encode(string)\n",
    "decoded = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "print('\"'+decoded+'\"')\n",
    "len(tokens), len(string)"
   ],
   "id": "5e1112dbda2c7537",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hello!!! This is a test to see how well the tokenizer works. :)\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17, 63)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T23:08:39.854701Z",
     "start_time": "2024-08-13T23:08:39.846767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "32000 * 16"
   ],
   "id": "50ee3ffe01e52aba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T23:11:00.013913Z",
     "start_time": "2024-08-13T23:10:59.978447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "vocab[:10], vocab[-10:]"
   ],
   "id": "4ac98556f34269af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('<unk>', 0),\n",
       "  ('<s>', 1),\n",
       "  ('</s>', 2),\n",
       "  ('<0x00>', 3),\n",
       "  ('<0x01>', 4),\n",
       "  ('<0x02>', 5),\n",
       "  ('<0x03>', 6),\n",
       "  ('<0x04>', 7),\n",
       "  ('<0x05>', 8),\n",
       "  ('<0x06>', 9)],\n",
       " [('ὀ', 31990),\n",
       "  ('げ', 31991),\n",
       "  ('べ', 31992),\n",
       "  ('边', 31993),\n",
       "  ('还', 31994),\n",
       "  ('黃', 31995),\n",
       "  ('왕', 31996),\n",
       "  ('收', 31997),\n",
       "  ('弘', 31998),\n",
       "  ('给', 31999)])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T23:13:21.189028Z",
     "start_time": "2024-08-13T23:13:21.173962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab[-2000:-1900]"
   ],
   "id": "7c5cdc89f43f49da",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('è', 30000),\n",
       " ('à', 30001),\n",
       " ('ш', 30002),\n",
       " ('—', 30003),\n",
       " ('\\r', 30004),\n",
       " ('ю', 30005),\n",
       " ('ł', 30006),\n",
       " ('»', 30007),\n",
       " ('С', 30008),\n",
       " ('«', 30009),\n",
       " ('’', 30010),\n",
       " ('ф', 30011),\n",
       " ('В', 30012),\n",
       " ('П', 30013),\n",
       " ('К', 30014),\n",
       " ('“', 30015),\n",
       " ('ј', 30016),\n",
       " ('М', 30017),\n",
       " ('А', 30018),\n",
       " ('ç', 30019),\n",
       " ('å', 30020),\n",
       " ('щ', 30021),\n",
       " ('~', 30022),\n",
       " ('ę', 30023),\n",
       " ('”', 30024),\n",
       " ('ą', 30025),\n",
       " ('č', 30026),\n",
       " ('Р', 30027),\n",
       " ('ї', 30028),\n",
       " ('Н', 30029),\n",
       " ('ú', 30030),\n",
       " ('Б', 30031),\n",
       " ('Д', 30032),\n",
       " ('ã', 30033),\n",
       " ('ß', 30034),\n",
       " ('ă', 30035),\n",
       " ('ě', 30036),\n",
       " ('ê', 30037),\n",
       " ('О', 30038),\n",
       " ('š', 30039),\n",
       " ('Г', 30040),\n",
       " ('Т', 30041),\n",
       " ('ż', 30042),\n",
       " ('ё', 30043),\n",
       " ('ž', 30044),\n",
       " ('ś', 30045),\n",
       " ('ñ', 30046),\n",
       " ('ř', 30047),\n",
       " ('ő', 30048),\n",
       " ('„', 30049),\n",
       " ('Л', 30050),\n",
       " ('э', 30051),\n",
       " ('ý', 30052),\n",
       " ('У', 30053),\n",
       " ('И', 30054),\n",
       " ('ъ', 30055),\n",
       " ('є', 30056),\n",
       " ('â', 30057),\n",
       " ('î', 30058),\n",
       " ('ò', 30059),\n",
       " ('З', 30060),\n",
       " ('Ф', 30061),\n",
       " ('É', 30062),\n",
       " ('ć', 30063),\n",
       " ('·', 30064),\n",
       " ('ș', 30065),\n",
       " ('ń', 30066),\n",
       " ('ț', 30067),\n",
       " ('Х', 30068),\n",
       " ('ô', 30069),\n",
       " ('Е', 30070),\n",
       " ('ù', 30071),\n",
       " ('ů', 30072),\n",
       " ('°', 30073),\n",
       " ('Ш', 30074),\n",
       " ('љ', 30075),\n",
       " ('Ч', 30076),\n",
       " ('ø', 30077),\n",
       " ('æ', 30078),\n",
       " ('њ', 30079),\n",
       " ('\\u2009', 30080),\n",
       " ('\\xa0', 30081),\n",
       " ('Э', 30082),\n",
       " ('ë', 30083),\n",
       " ('õ', 30084),\n",
       " ('ï', 30085),\n",
       " ('‘', 30086),\n",
       " ('†', 30087),\n",
       " ('²', 30088),\n",
       " ('ű', 30089),\n",
       " ('І', 30090),\n",
       " ('─', 30091),\n",
       " ('Ц', 30092),\n",
       " ('ћ', 30093),\n",
       " ('Ö', 30094),\n",
       " ('û', 30095),\n",
       " ('Я', 30096),\n",
       " ('ì', 30097),\n",
       " ('…', 30098),\n",
       " ('ō', 30099)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T23:12:38.128814Z",
     "start_time": "2024-08-13T23:12:38.122081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer.decode([tok[1] for tok in vocab[-10000:-9990]])"
   ],
   "id": "eab3d4ebccb969ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iframe concepts tack viss carbontery naming Ortsidente Capit'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Surgery on tokenizer"
   ],
   "id": "340f9db1157a161a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T23:14:59.245214Z",
     "start_time": "2024-08-13T23:14:59.006117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mini_tokenizer = transformers.LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "len(mini_tokenizer)"
   ],
   "id": "a85f3a1c8cce098f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-13T23:16:06.543304Z",
     "start_time": "2024-08-13T23:16:06.535482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mini_tokenizer.vocab_files_names"
   ],
   "id": "99e292c73d447b27",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_file': 'tokenizer.model', 'tokenizer_file': 'tokenizer.json'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mini_tokenizer.get_vocab()"
   ],
   "id": "cafedb6097097bbb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
