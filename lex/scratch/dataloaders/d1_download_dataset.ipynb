{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# FineWeb-Edu 10B subset\n",
    "\n",
    "https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu?row=5"
   ],
   "id": "254602cb0bc40099"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T03:25:24.370415Z",
     "start_time": "2024-08-17T03:25:24.363548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset  # pip install datasets"
   ],
   "id": "22b152b897823920",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T14:43:45.724665Z",
     "start_time": "2024-08-09T14:33:44.965441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "download_dir = \"/mnt/md0/data\"\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\", cache_dir=download_dir)"
   ],
   "id": "68f96e1729be48c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/22.9k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9fcd0a4bf5a4050a6d7a41f529d640d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/1630 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e266b048b544e7c8beb04a203e046ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec3ac4ffa8024701a104d7ba41375fc2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d22d01d940fe4138a9e97accf743db0f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "621b369ded8a43ae932bd7c69fdde845"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee388817f7714d87a5fb28efccdc255b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "018ec4f0ee7e4ca6874e5e7cd23a93eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef431ef997e142058095b4f6e14c8009"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3fa939aef71441294a51bdb59bbdcf2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "075ad83bc04144f38dc81ce474272e80"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "74f43610d99345ddbfec30c07d9e1944"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7945d82affd149a98fac56c13d25e756"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8be8741ff1c14f95842562c8c6d19334"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89f91f2390e7405c9b93461d2b0ecc6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38d8793950ae4fd5b69f20d2162ae362"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/541M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3337a01b8217417391351d8e4596bcab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/9672101 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cafeff137d384b2591a88f9027d1ea4d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4b5599c990d422fbe9358db9bb8e361"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T14:45:42.686508Z",
     "start_time": "2024-08-09T14:45:42.678426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fw"
   ],
   "id": "2a443940f1312038",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'dump', 'url', 'file_path', 'language', 'language_score', 'token_count', 'score', 'int_score'],\n",
       "    num_rows: 9672101\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T14:54:33.329247Z",
     "start_time": "2024-08-09T14:54:33.320601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fw[1]"
   ],
   "id": "7fe0fb2e20ce80eb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Taking Play Seriously\\nBy ROBIN MARANTZ HENIG\\nPublished: February 17, 2008\\nOn a drizzly Tuesday night in late January, 200 people came out to hear a psychiatrist talk rhapsodically about play -- not just the intense, joyous play of children, but play for all people, at all ages, at all times. (All species too; the lecture featured touching photos of a polar bear and a husky engaging playfully at a snowy outpost in northern Canada.) Stuart Brown, president of the National Institute for Play, was speaking at the New York Public Library's main branch on 42nd Street. He created the institute in 1996, after more than 20 years of psychiatric practice and research persuaded him of the dangerous long-term consequences of play deprivation. In a sold-out talk at the library, he and Krista Tippett, host of the public-radio program ''Speaking of Faith,'' discussed the biological and spiritual underpinnings of play. Brown called play part of the ''developmental sequencing of becoming a human primate. If you look at what produces learning and memory and well-being, play is as fundamental as any other aspect of life, including sleep and dreams.''\\nThe message seemed to resonate with audience members, who asked anxious questions about what seemed to be the loss of play in their children's lives. Their concern came, no doubt, from the recent deluge of eulogies to play . Educators fret that school officials are hacking away at recess to make room for an increasingly crammed curriculum. Psychologists complain that overscheduled kids have no time left for the real business of childhood: idle, creative, unstructured free play. Public health officials link insufficient playtime to a rise in childhood obesity. Parents bemoan the fact that kids don't play the way they themselves did -- or think they did. And everyone seems to worry that without the chance to play stickball or hopscotch out on the street, to play with dolls on the kitchen floor or climb trees in the woods, today's children are missing out on something essential.\\nThe success of ''The Dangerous Book for Boys'' -- which has been on the best-seller list for the last nine months -- and its step-by-step instructions for activities like folding paper airplanes is testament to the generalized longing for play's good old days. So were the questions after Stuart Brown's library talk; one woman asked how her children will learn trust, empathy and social skills when their most frequent playing is done online. Brown told her that while video games do have some play value, a true sense of ''interpersonal nuance'' can be achieved only by a child who is engaging all five senses by playing in the three-dimensional world.\\nThis is part of a larger conversation Americans are having about play. Parents bobble between a nostalgia-infused yearning for their children to play and fear that time spent playing is time lost to more practical pursuits. Alarming headlines about U.S. students falling behind other countries in science and math, combined with the ever-more-intense competition to get kids into college, make parents rush to sign up their children for piano lessons and test-prep courses instead of just leaving them to improvise on their own; playtime versus r?m?uilding.\\nDiscussions about play force us to reckon with our underlying ideas about childhood, sex differences, creativity and success. Do boys play differently than girls? Are children being damaged by staring at computer screens and video games? Are they missing something when fantasy play is populated with characters from Hollywood's imagination and not their own? Most of these issues are too vast to be addressed by a single field of study (let alone a magazine article). But the growing science of play does have much to add to the conversation. Armed with research grounded in evolutionary biology and experimental neuroscience, some scientists have shown themselves eager -- at times perhaps a little too eager -- to promote a scientific argument for play. They have spent the past few decades learning how and why play evolved in animals, generating insights that can inform our understanding of its evolution in humans too. They are studying, from an evolutionary perspective, to what extent play is a luxury that can be dispensed with when there are too many other competing claims on the growing brain, and to what extent it is central to how that brain grows in the first place.\\nScientists who study play, in animals and humans alike, are developing a consensus view that play is something more than a way for restless kids to work off steam; more than a way for chubby kids to burn off calories; more than a frivolous luxury. Play, in their view, is a central part of neurological growth and development -- one important way that children build complex, skilled, responsive, socially adept and cognitively flexible brains.\\nTheir work still leaves some questions unanswered, including questions about play's darker, more ambiguous side: is there really an evolutionary or developmental need for dangerous games, say, or for the meanness and hurt feelings that seem to attend so much child's play? Answering these and other questions could help us understand what might be lost if children play less.\",\n",
       " 'id': '<urn:uuid:316c7af5-14e1-4d0b-9576-753e17ef2cc5>',\n",
       " 'dump': 'CC-MAIN-2013-20',\n",
       " 'url': 'http://query.nytimes.com/gst/fullpage.html?res=9404E7DA1339F934A25751C0A96E9C8B63&scp=2&sq=taking%20play%20seriously&st=cse',\n",
       " 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz',\n",
       " 'language': 'en',\n",
       " 'language_score': 0.9614589214324951,\n",
       " 'token_count': 1055,\n",
       " 'score': 2.5625,\n",
       " 'int_score': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tokenize the dataset"
   ],
   "id": "921a2d46b7e2147e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T01:28:42.190225Z",
     "start_time": "2024-08-16T01:28:42.040912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import LlamaTokenizerFast\n",
    "enc = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\") # 32000\n",
    "enc"
   ],
   "id": "deeb01f5692283b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='hf-internal-testing/llama-tokenizer', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T01:53:49.460462Z",
     "start_time": "2024-08-16T01:53:49.439913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded = enc.encode(fw[1][\"text\"])\n",
    "decoded = enc.decode(encoded)\n",
    "len(encoded), len(decoded), decoded == fw[1][\"text\"]"
   ],
   "id": "be10f80a9e519139",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1206, 5259, False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T01:54:26.304391Z",
     "start_time": "2024-08-16T01:54:26.294398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoded[:100], enc.tokenize(decoded)[:5], enc.tokenize(fw[1][\"text\"])[:5]"
   ],
   "id": "d9faf890ebf74854",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s> Taking Play Seriously\\nBy ROBIN MARANTZ HENIG\\nPublished: February 17, 2008\\nOn a drizzly Tuesday n',\n",
       " ['▁<s>', '▁T', 'aking', '▁Play', '▁Ser'],\n",
       " ['▁T', 'aking', '▁Play', '▁Ser', 'iously'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T01:31:01.303242Z",
     "start_time": "2024-08-16T01:31:01.295232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fw[1][\"text\"][:100]"
   ],
   "id": "fc069908835d7664",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Taking Play Seriously\\nBy ROBIN MARANTZ HENIG\\nPublished: February 17, 2008\\nOn a drizzly Tuesday night'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T01:39:10.559767Z",
     "start_time": "2024-08-16T01:39:10.551690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Search all occurances of enc.special_tokens_map[\"eos_token\"] in decoded\n",
    "import re\n",
    "for k, v in enc.special_tokens_map.items():\n",
    "    print(k, v, len(re.findall(v, decoded)))"
   ],
   "id": "95e91d91647fe6ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token <s> 1\n",
      "eos_token </s> 0\n",
      "unk_token <unk> 0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T01:40:41.197979Z",
     "start_time": "2024-08-16T01:40:41.179030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded = enc.encode(fw[1][\"text\"], add_special_tokens=False)\n",
    "decoded = enc.decode(encoded)\n",
    "decoded[:100], decoded == fw[1][\"text\"]"
   ],
   "id": "c998e5ac2866bc22",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Taking Play Seriously\\nBy ROBIN MARANTZ HENIG\\nPublished: February 17, 2008\\nOn a drizzly Tuesday night',\n",
       " True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T02:53:03.385229Z",
     "start_time": "2024-08-16T02:53:03.372060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokenize(document):\n",
    "    tokens = enc.encode(enc.special_tokens_map[\"bos_token\"], add_special_tokens=False)\n",
    "    tokens.extend(enc.encode(document[\"text\"], add_special_tokens=False))\n",
    "    tokens_np = np.asarray(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
    "    # 2**16 = 65536 which is always true for llama tokenizer with 32000 vocab size\n",
    "    tokens_np = tokens_np.astype(np.uint16)\n",
    "    return tokens_np\n",
    "\n",
    "\n",
    "tokenize(fw[1])"
   ],
   "id": "b59c240529a54ffe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,   323,  5086, ...,  1708,  3109, 29889], dtype=uint16)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Sharding\n",
    "\n",
    "All documents become a massive array of uint16 tokens.\n",
    "We can shard this array into smaller arrays and save them as numpy arrays.\n",
    "We can also save the metadata in a separate file.\n",
    "\n",
    "Now every shard will have roughly the same number of tokens.\n",
    "They will not contain incomplete documents.\n",
    "To shuffle the dataset, we can shuffle the order of the shards and then shuffle the documents within each shard."
   ],
   "id": "74239427db6b318d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T04:41:53.502019Z",
     "start_time": "2024-08-17T04:03:16.550613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "from os import makedirs, cpu_count, environ\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "max_tokens_per_shard = 100_000_000  # 100M tokens, 191mb, 84k docs\n",
    "max_shards = np.inf\n",
    "save_dir = \"/mnt/ssd/data/fineweb-edu-10BT/llama-tokenizer\"\n",
    "makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Metadata Initialization\n",
    "shard_metadata_template = {\"source_indices\": [], \"bos_indices\": [], \"num_tokens\": 0, \"num_documents\": 0}\n",
    "complete_metadata = {\n",
    "    \"dataset_name\": \"fineweb-edu-10BT\",\n",
    "    \"tokenizer_name\": \"hf-internal-testing/llama-tokenizer\",\n",
    "    \"max_tokens_per_shard\": max_tokens_per_shard,\n",
    "    \"num_training_tokens\": 0,\n",
    "    \"train_shards\": [],  # Contains indices of shards\n",
    "    \"validation_shards\": [],\n",
    "    \"test_shards\": [],\n",
    "    \"num_shards\": 0,\n",
    "    \"num_tokens\": [],  # contains number of tokens in each shard\n",
    "    \"num_documents\": [],  # contains number of documents in each shard\n",
    "}\n",
    "\n",
    "# Helper function to save shard data\n",
    "def save_shard_data(shard_idx, tokens, metadata):\n",
    "    np.save(f\"{save_dir}/tokens_shard_{shard_idx:03d}.npy\", tokens)\n",
    "    with open(f\"{save_dir}/metadata_shard_{shard_idx:03d}.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    complete_metadata[\"num_tokens\"].append(metadata[\"num_tokens\"])\n",
    "    complete_metadata[\"num_documents\"].append(metadata[\"num_documents\"])\n",
    "\n",
    "# Main processing loop\n",
    "current_document_index = 0\n",
    "current_shard_index = 0\n",
    "current_shard_tokens = 0\n",
    "shard_tokens = np.zeros(max_tokens_per_shard, dtype=np.uint16)\n",
    "shard_metadata = deepcopy(shard_metadata_template)\n",
    "\n",
    "n_procs = max(1, int(cpu_count() * 0.5))  # 128 on MJ\n",
    "with Pool(n_procs) as pool:\n",
    "    # Without multiprocessing\n",
    "    # for i, tokens_np in enumerate(tqdm(map(tokenize, fw), total=len(fw))):\n",
    "    # 100%|██████████| 9672101/9672101 [41:01<00:00, 3929.23it/s] with 128 procs\n",
    "    for i, tokens_np in enumerate(tqdm(pool.imap(tokenize, fw), total=len(fw))):\n",
    "        if current_shard_tokens + len(tokens_np) > max_tokens_per_shard:\n",
    "            save_shard_data(current_shard_index, shard_tokens[:current_shard_tokens], shard_metadata)\n",
    "            current_shard_index += 1\n",
    "            current_shard_tokens = 0\n",
    "            shard_tokens.fill(0)\n",
    "            shard_metadata = deepcopy(shard_metadata_template)\n",
    "            complete_metadata[\"num_shards\"] += 1\n",
    "            if current_shard_index >= max_shards:\n",
    "                break\n",
    "\n",
    "        shard_tokens[current_shard_tokens:current_shard_tokens + len(tokens_np)] = tokens_np\n",
    "        shard_metadata[\"bos_indices\"].append(current_shard_tokens)\n",
    "        current_shard_tokens += len(tokens_np)\n",
    "        shard_metadata[\"source_indices\"].append(i)\n",
    "        shard_metadata[\"num_tokens\"] += len(tokens_np)\n",
    "        shard_metadata[\"num_documents\"] += 1\n",
    "        current_document_index += 1\n",
    "\n",
    "    if current_shard_tokens > 0:\n",
    "        save_shard_data(current_shard_index, shard_tokens[:current_shard_tokens], shard_metadata)\n",
    "\n",
    "# Split the shards into train, validation, and test in 90:5:5 ratio\n",
    "val_shard_start = int(complete_metadata[\"num_shards\"] * 0.9)\n",
    "test_shard_start = int(complete_metadata[\"num_shards\"] * 0.95)\n",
    "# Shuffle the shards\n",
    "shard_indices = np.arange(complete_metadata[\"num_shards\"])\n",
    "np.random.shuffle(shard_indices)\n",
    "complete_metadata[\"train_shards\"] = shard_indices[:val_shard_start].tolist()\n",
    "complete_metadata[\"validation_shards\"] = shard_indices[val_shard_start:test_shard_start].tolist()\n",
    "complete_metadata[\"test_shards\"] = shard_indices[test_shard_start:].tolist()\n",
    "complete_metadata[\"num_training_tokens\"] = sum([complete_metadata[\"num_tokens\"][i] for i in complete_metadata[\"train_shards\"]])\n",
    "\n",
    "# Save the complete metadata as json\n",
    "with open(f\"{save_dir}/metadata.json\", \"w\") as f:\n",
    "    json.dump(complete_metadata, f, indent=2)\n",
    "    \n",
    "complete_metadata"
   ],
   "id": "93cd5edd43db962d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9672101/9672101 [38:20<00:00, 4203.53it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'fineweb-edu-10BT',\n",
       " 'tokenizer_name': 'hf-internal-testing/llama-tokenizer',\n",
       " 'max_tokens_per_shard': 100000000,\n",
       " 'num_training_tokens': 10199788391,\n",
       " 'train_shards': [7,\n",
       "  75,\n",
       "  107,\n",
       "  31,\n",
       "  62,\n",
       "  11,\n",
       "  19,\n",
       "  50,\n",
       "  91,\n",
       "  66,\n",
       "  86,\n",
       "  13,\n",
       "  5,\n",
       "  68,\n",
       "  43,\n",
       "  93,\n",
       "  73,\n",
       "  87,\n",
       "  72,\n",
       "  46,\n",
       "  15,\n",
       "  113,\n",
       "  41,\n",
       "  76,\n",
       "  10,\n",
       "  60,\n",
       "  96,\n",
       "  8,\n",
       "  28,\n",
       "  52,\n",
       "  74,\n",
       "  79,\n",
       "  3,\n",
       "  26,\n",
       "  32,\n",
       "  23,\n",
       "  70,\n",
       "  105,\n",
       "  25,\n",
       "  89,\n",
       "  53,\n",
       "  6,\n",
       "  18,\n",
       "  17,\n",
       "  100,\n",
       "  104,\n",
       "  4,\n",
       "  21,\n",
       "  34,\n",
       "  51,\n",
       "  22,\n",
       "  35,\n",
       "  12,\n",
       "  82,\n",
       "  78,\n",
       "  57,\n",
       "  88,\n",
       "  58,\n",
       "  98,\n",
       "  45,\n",
       "  0,\n",
       "  24,\n",
       "  65,\n",
       "  47,\n",
       "  36,\n",
       "  14,\n",
       "  109,\n",
       "  111,\n",
       "  103,\n",
       "  30,\n",
       "  54,\n",
       "  85,\n",
       "  95,\n",
       "  2,\n",
       "  59,\n",
       "  49,\n",
       "  42,\n",
       "  94,\n",
       "  16,\n",
       "  29,\n",
       "  61,\n",
       "  38,\n",
       "  56,\n",
       "  80,\n",
       "  92,\n",
       "  112,\n",
       "  97,\n",
       "  48,\n",
       "  81,\n",
       "  90,\n",
       "  33,\n",
       "  63,\n",
       "  39,\n",
       "  69,\n",
       "  83,\n",
       "  44,\n",
       "  20,\n",
       "  37,\n",
       "  64,\n",
       "  108,\n",
       "  9,\n",
       "  40],\n",
       " 'validation_shards': [106, 110, 1, 101, 102, 71],\n",
       " 'test_shards': [27, 99, 55, 84, 67, 77],\n",
       " 'num_shards': 114,\n",
       " 'num_tokens': [99999023,\n",
       "  99989972,\n",
       "  99999006,\n",
       "  99997050,\n",
       "  99998535,\n",
       "  99999923,\n",
       "  99999947,\n",
       "  99998305,\n",
       "  99999775,\n",
       "  99998573,\n",
       "  99999946,\n",
       "  99999423,\n",
       "  99999938,\n",
       "  99999867,\n",
       "  99999602,\n",
       "  99998001,\n",
       "  99998504,\n",
       "  99998743,\n",
       "  99986263,\n",
       "  99999861,\n",
       "  99999242,\n",
       "  99996920,\n",
       "  99999170,\n",
       "  99999076,\n",
       "  99988660,\n",
       "  99999804,\n",
       "  99995469,\n",
       "  99998954,\n",
       "  99999008,\n",
       "  99999581,\n",
       "  99999701,\n",
       "  99999831,\n",
       "  99998917,\n",
       "  99999656,\n",
       "  99999828,\n",
       "  99999395,\n",
       "  99999482,\n",
       "  99999267,\n",
       "  99999853,\n",
       "  99999801,\n",
       "  99999732,\n",
       "  99997597,\n",
       "  99999789,\n",
       "  99999997,\n",
       "  99999336,\n",
       "  99999916,\n",
       "  99999222,\n",
       "  99992313,\n",
       "  99999640,\n",
       "  99999431,\n",
       "  99999201,\n",
       "  99999844,\n",
       "  99974207,\n",
       "  99999734,\n",
       "  99999834,\n",
       "  99997320,\n",
       "  99998362,\n",
       "  99998850,\n",
       "  99999960,\n",
       "  99999009,\n",
       "  99999978,\n",
       "  99994129,\n",
       "  99999891,\n",
       "  99998737,\n",
       "  99999407,\n",
       "  99999600,\n",
       "  99995047,\n",
       "  99999595,\n",
       "  99999970,\n",
       "  99999127,\n",
       "  99999270,\n",
       "  99999024,\n",
       "  99998825,\n",
       "  99999397,\n",
       "  99999774,\n",
       "  99997959,\n",
       "  99999414,\n",
       "  99999598,\n",
       "  99999951,\n",
       "  99999092,\n",
       "  99997183,\n",
       "  99999104,\n",
       "  99984496,\n",
       "  99999028,\n",
       "  99995115,\n",
       "  99997199,\n",
       "  99999854,\n",
       "  99999828,\n",
       "  99998134,\n",
       "  99993814,\n",
       "  99998540,\n",
       "  99995589,\n",
       "  99999518,\n",
       "  99996094,\n",
       "  99998250,\n",
       "  99999591,\n",
       "  99999346,\n",
       "  99998177,\n",
       "  99991962,\n",
       "  99999492,\n",
       "  99981723,\n",
       "  99999975,\n",
       "  99986178,\n",
       "  99999865,\n",
       "  99998126,\n",
       "  99999632,\n",
       "  99998924,\n",
       "  99999400,\n",
       "  99999410,\n",
       "  99996686,\n",
       "  99999127,\n",
       "  99989462,\n",
       "  99999997,\n",
       "  99999895,\n",
       "  13012411],\n",
       " 'num_documents': [83021,\n",
       "  85175,\n",
       "  86129,\n",
       "  84861,\n",
       "  82873,\n",
       "  83415,\n",
       "  83977,\n",
       "  84416,\n",
       "  83841,\n",
       "  86217,\n",
       "  85276,\n",
       "  86481,\n",
       "  83795,\n",
       "  83781,\n",
       "  84323,\n",
       "  84405,\n",
       "  84438,\n",
       "  83027,\n",
       "  84816,\n",
       "  84798,\n",
       "  85405,\n",
       "  83984,\n",
       "  84413,\n",
       "  84760,\n",
       "  83241,\n",
       "  84427,\n",
       "  84282,\n",
       "  86042,\n",
       "  84693,\n",
       "  86943,\n",
       "  84943,\n",
       "  86514,\n",
       "  85495,\n",
       "  83904,\n",
       "  85897,\n",
       "  86294,\n",
       "  86704,\n",
       "  87233,\n",
       "  88221,\n",
       "  85495,\n",
       "  86743,\n",
       "  87068,\n",
       "  86734,\n",
       "  86353,\n",
       "  87352,\n",
       "  85690,\n",
       "  85478,\n",
       "  86024,\n",
       "  85849,\n",
       "  84560,\n",
       "  84773,\n",
       "  85813,\n",
       "  85108,\n",
       "  85263,\n",
       "  84659,\n",
       "  84374,\n",
       "  83588,\n",
       "  84930,\n",
       "  85096,\n",
       "  84659,\n",
       "  84989,\n",
       "  85860,\n",
       "  84396,\n",
       "  85161,\n",
       "  85474,\n",
       "  83934,\n",
       "  81641,\n",
       "  82293,\n",
       "  82869,\n",
       "  83371,\n",
       "  84499,\n",
       "  84476,\n",
       "  84763,\n",
       "  85048,\n",
       "  84169,\n",
       "  83826,\n",
       "  83221,\n",
       "  82286,\n",
       "  83227,\n",
       "  82246,\n",
       "  80605,\n",
       "  82089,\n",
       "  82089,\n",
       "  82802,\n",
       "  83526,\n",
       "  83864,\n",
       "  83699,\n",
       "  84893,\n",
       "  85113,\n",
       "  84087,\n",
       "  84480,\n",
       "  85783,\n",
       "  84862,\n",
       "  84369,\n",
       "  84387,\n",
       "  86217,\n",
       "  85840,\n",
       "  84043,\n",
       "  85533,\n",
       "  86420,\n",
       "  85282,\n",
       "  85532,\n",
       "  85551,\n",
       "  85532,\n",
       "  86217,\n",
       "  84775,\n",
       "  84358,\n",
       "  85346,\n",
       "  85174,\n",
       "  85209,\n",
       "  84220,\n",
       "  85076,\n",
       "  84240,\n",
       "  84345,\n",
       "  10725]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "! ls /mnt/ssd/data/fineweb-edu-10BT/llama-tokenizer"
   ],
   "id": "51b85ee1f2779017",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
